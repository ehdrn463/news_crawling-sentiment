# 주피터 노트북으로 실행하세요
# print("===============") 기준으로 셀 분류하시면 됩니다.
print('=========================================================')
import time
import datetime
import os
import json

#크롤링
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common import exceptions
from collections import defaultdict

#Konly 형태소 분류
from konlpy.tag import Kkma
from konlpy.tag import Hannanum
from konlpy.utils import pprint

#감정분석
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as Vader

#워드클라우드
from krwordrank.word import KRWordRank
from wordcloud import WordCloud


print('=========================================================')
# Result 폴더 설정
if not os.path.isdir("./Result/"):
    os.mkdir("./Result/")
    print('making directory')


print('=========================================================')
wd = "./chromedriver.exe"  # 다운 받은 웹드라이버 위치
addr = "https://n.news.naver.com/article/comment/023/0003444614" 
# 크롤링하고자하는 사이트 주소
# 개인적으론 모바일 페이지로 하는게 더 가볍고 빠를것같은 기분이 든다.

driver = webdriver.Chrome(wd)
driver.get(addr)

pages = 0 # 한 페이지당 약 20개의 댓글이 표시
try:
    while True: # 댓글 페이지가 몇개인지 모르므로.
        time.sleep(5)
        driver.find_element_by_css_selector(".u_cbox_btn_more").click()
        #time.sleep(10)
        print(pages, end=" ")
        pages+=1
    
except exceptions.ElementNotVisibleException as e: # 페이지 끝
    print('댓글 다 펼쳤어')
    pass
    
except Exception as e: # 다른 예외 발생시 확인
    print(e)




print('=========================================================')
# 댓글이 들어있는 페이지 전체 크롤링    
html = driver.page_source
dom = BeautifulSoup(html, "lxml")
comments_raw = dom.find_all("span", {"class" : "u_cbox_contents"})

json_list=list()
json_data=dict()

for comment_raw in comments_raw:
    comment = comment_raw.text
    json_data['comment']=comment
    json_list.append(comment)
print('===========================================================================')

#크롤링한 결과 
def makeJsonFile(dictionList):
    jsonFileName = 'Joseon_comment.json'
    with open(jsonFileName, 'a', encoding="utf-8") as make_file:        
        if ( isinstance(dictionList, list) ):
            json.dump(dictionList, make_file, ensure_ascii=False, indent="\t")
            return jsonFileName
        
makeJsonFile(json_list)


print('=========================================================')
hannanum = Hannanum()
print(hannanum.nouns(u'다람쥐 헌 쳇바퀴에 타고파'))


print('=========================================================')
def animate(crawling_comment):
    pullData=open(crawling_comment, encoding='utf-8', mode='r').read()
    lines=pullData.split(",")
    return(lines)
    print(len(lines))
animate('./Joseon_comment.json')


print('=========================================================')
def divide(file_data):
    comment_list=[]
    for i in range(len(file_data)):
        comment_list.append(hannanum.nouns(file_data[i]))
    return comment_list

    
divide(animate('./Joseon_comment.json'))


print('=========================================================')
wordnames = divide(animate('./Joseon_comment.json'))
for a in range(len(wordnames)):
    for b in wordnames[a]:
        
        
        with open('SentiWord_info.json', encoding='utf-8-sig', mode='r') as f:
            data = json.load(f)
        result = ['None','None']
        for i in range(0, len(data)):
            if data[i]['word'] == b:
                result.pop()
                result.pop()
                result.append(data[i]['word_root'])
                result.append(data[i]['polarity'])

        r_word = result[0]
        s_word = result[1]
        print(b)
        print('어근 : ' + r_word)
        print('극성 : ' + s_word)


        #return r_word, s_word


print('=========================================================')
class KnuSL():

    def data_list(wordname):
        with open('SentiWord_info.json', encoding='utf-8-sig', mode='r') as f:
            data = json.load(f)
        result = ['None','None']
        for i in range(0, len(data)):
            if data[i]['word'] == wordname:
                result.pop()
                result.pop()
                result.append(data[i]['word_root'])
                result.append(data[i]['polarity'])

        r_word = result[0]
        s_word = result[1]

        print('어근 : ' + r_word)
        print('극성 : ' + s_word)


        return r_word, s_word

    
    
if __name__ == "__main__":
    ksl = KnuSL

    print("\nKNU 한국어 감성사전입니다~ :)")
    print("사전에 단어가 없는 경우 결과가 None으로 나타납니다!!!")
    print("-2:매우 부정, -1:부정, 0:중립 or Unkwon, 1:긍정, 2:매우 긍정")
    print("\n")	

    wordnames = divide(animate('./Joseon_comment.json'))
    for i in range(len(wordnames)):
        for j in wordnames[i]:
            print(j)
            print(ksl.data_list(j))
        
       
  
        
